Microsoft Malware Prediction
======================
This is my solution to the problem posted as part of the  [kaggle competition on predicting the occurence of malware on computer systems.](https://www.kaggle.com/c/microsoft-malware-prediction/)

  ![image](https://github.com/babinu-uthup-4JESUS/Kaggle_Microsoft_Malware_Prediction/blob/master/rel_images/microsoft_malware_prediction_comp.png)

## Table of content

- [Overview](#overview)
- [Data](#data)
    - [Data Description](#data-description)
- [Approach](#approach)
    - [First look at data](#first-look-at-data)

## Overview

We have the following problem description from it's [corresponding kaggle competition link](https://www.kaggle.com/c/microsoft-malware-prediction/overview/description) :

>The malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways.

>With more than one billion enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security.

>As one part of their overall strategy for doing so, Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous, Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences.

>Can you help protect more than one billion machines from damage BEFORE it happens?

## Data

All the information pasted in this section has been obtained from the [this webpage (which shows data related information of the corresponding kaggle competition).](https://www.kaggle.com/c/microsoft-malware-prediction/data)


### Data Description
 
>Each row in this dataset corresponds to a machine, uniquely identified by a MachineIdentifier. HasDetections is the ground truth and indicates that Malware was detected on the machine. Using the information and labels in train.csv, you must predict the value for HasDetections for each machine in test.csv.

Further description of each of the columns in the data can be obtained from the [link on kaggle.](https://www.kaggle.com/c/microsoft-malware-prediction/data)


## Approach

The approach is to have a thorough look at the data, validate the same and decide on the path ahead.

### First look at data

We had a first look at the data and explored a simple approach based on [conditional probablities.](https://github.com/babinu-uthup-4JESUS/Kaggle_Microsoft_Malware_Prediction/tree/master/first_look/first_look.ipynb) Though the method looked promising initially, it did not yield us any meaningful progress in the end.  

### Detailed look

We took a [scrupulous look at the data and fixed some of the missing values as well.](https://github.com/babinu-uthup-4JESUS/Kaggle_Microsoft_Malware_Prediction/tree/master/more_analysis/more_analysis.ipynb) To make the analysis more cogent, we also inspected data from the validation and test sets to ensure that they were indeed coming from the same distribution.

### Logistic regression

By employing [this techique, coupled with a greedy approach for variable selection](https://github.com/babinu-uthup-4JESUS/Kaggle_Microsoft_Malware_Prediction/tree/master/modeling_approaches/logistic_regression/logistic_regression.ipynb), we  were able to prop up the validation score to 0.607.

### Gradient Boosting using light gbm

Similar to logistic regression, [we employed gradient boosting using lightgbm, coupled with a greedy approach](https://github.com/babinu-uthup-4JESUS/Kaggle_Microsoft_Malware_Prediction/tree/master/modeling_approaches/gradient_boosting/gradient_boosting_build_final_model_on_full_train.ipynb), and were able to prop up the validation error to 0.67 (For improved performance, [the relevant script was run as a kaggle kernel](https://www.kaggle.com/babinu/microsoft-malware-prediction-lightgbm))

### Gradient Boosting using light gbm using frequency encoding and extra data by Christ Deott

Similar to logistic regression, [we employed gradient boosting using lightgbm, coupled with a greedy approach](https://github.com/babinu-uthup-4JESUS/Kaggle_Microsoft_Malware_Prediction/tree/master/modeling_approaches/gradient_boosting/gradient_boosting_build_final_model_on_full_train_deott_inspire.ipynb), and were able to prop up the validation error to 0.69 (For improved performance, [the relevant script was run as a series kaggle kernels](https://www.kaggle.com/babinu/parallel-frequency-encod-1))

### Conclusion

Though we were able get a good score on our validation set, this did not translate to a score of a similar magnitude on the test set owing to reasons mentinoed [here](https://www.kaggle.com/c/microsoft-malware-prediction/discussion/84745). However,  the very fact that we were able to do data analysis, frequency encoding and build an efficient lgbm model is commendable and stands out as an important lesson from the project.
